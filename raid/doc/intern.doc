-*- Mode: indented-text -*-
--- $Id: intern.doc,v 1.8 2004/09/11 17:29:47 vitus Exp $

    Visual RAID Filter - Treiber Dokumentation

Durchzufhrende Tests

    Ausfall im Build

	Neues Array erzeugen, rebooten.  Dann eine Platte abziehen.
	Das Array sollte entweder sofort in den ERROR State gehen
	(einfachste L”sung) oder den Build ohne dieses Kind
	fortsetzen.  Wenn die abgezogene Platte das Kind mit den
	gltigen Daten ("Master") war, dann muá ein neuer Master
	definiert wrden und der Build Prozeá mit ihm neu gestartet
	werden.

    Ausfall im Rebuild

	Rebuild ber vsetup.exe startet, Platte abziehen (es muá nicht
	gebootet werden).  Der Rebuild muá sofort gestoppt werden und
	das Array in den Fail oder evtl Error (falls "declare as
	invalid" benutzt wurde) State gehen.


Speicherzugriffe

    Lineare Adressen k”nnen in 16:32 Zeiger umgewandelt werden, durch
    Verwendung von DOS32FLATDS.



Messungen

    Verwendete Benchmarks und ihre Characteristiken

    IoZone v1.09

	Schreibt eine groáe Datei sequentiell und liest sie dann,
	ebenfalls sequentiell.  Die Lese-Performance ist von der
	Schreib-Performance abh„ngig, da parallel zum Lesen den
	HPFS-Cache flusht.

    IoStone

	Liest doppelt soviel wie er schreibt.  Verteilung der I/Os
	unbekannt, das Ergebnis wird ber den Durchschnitt berechnet.

    PDB v0.01

	--------------------------------------------------
	The test is divided into 2 parts:

	Part 1:
	Sequential write to create the file
	Random read
	Random write with the same data

	Part 2:
	Sequential read of the entire file
	Random read
	Random write of changed data
	--------------------------------------------------

	Die Gesamtzeit des Vorgangs ergibt die Performance.  Die
	Lese-Performance wird *nicht* ausschlieslich durch den Flush
	bestimmt (in der Tat „ndern sich die Werte kaum, wenn man ein
	Cache Flush vor "Part 2" einfgt).



    22/6-1999 - 0.70 - 486DX-50 - HPFS 1536/32


	IoZone 8				write / read (KB/s)

	    IBM 150MB:				635 / 657
	    Stripe aus 3 IBM 150MB:		752 / 455
	    Mirror aus 2 IBM 150MB:		590 / 240
	    Quantum 510MB:			1028 / 287
	    Mirror aus 1 Quantum 510MB:		1043 / 686
	    Stripe aus 3 IBM 150MB,		507 / 258	    (build)
	     gespiegelt auf Quantum 510MB:      531 / 353	    (ready)


	IoStone					iostones/s

	    IBM 150MB:				32787-37038-38462
	    Stripe aus 3 IBM 150MB:		32259
	    Mirror aus 2 IBM 150MB:		31747-38462
	    Quantum 510MB:			37736
	    Mirror aus 1 Quantum 510MB:		31250-40817
	    Stripe aus 3 IBM 150MB,		29851		(build)
	     gespiegelt auf Quantum 510MB:      35715		(ready)


	PDB32 32 64				seconds

	    IBM 150MB:				127,6
	    Stripe aus 3 IBM 150MB:		123,8
	    Mirror aus 2 IBM 150MB:		116,0
	    Quantum 510MB:			66,6
	    Mirror aus 1 Quantum 510MB:		61,5
	    Stripe aus 3 IBM 150MB,		153,2		(build)
	     gespiegelt auf Quantum 510MB:      88.4		(ready)


	copy /b datei nul (64MB)

	    IBM 150MB:				867 KB/s
	    Stripe aus 3 IBM 150MB:
	    Quantum 510MB:
	    Mirror aus 1 Quantum 510MB:		1543 KB/s
	    Stripe aus 3 IBM 150MB,
	     gespiegelt auf Quantum 510MB:      1055 KB/s          (ready)


    19/4-98
	Stripe aus 2 IBM 150: 1.4 MB/s Lesen (96MB)
	Stripe aus 3 IBM 150: 1.4 MB/s Lesen (102MB)



Gleichzeitig FS und VRAID Partition in Benutzung

    Damit  ist    VRAID.FLT zus„tzlich   ein   „hnlicher   Filter  wie
    DSKSleep.flt.  Der Unterschied  besteht  darin, daá DSKSleep  sich
    als Filter eintr„gt, aber VRAID.FLT schon  das ganze Device belegt
    und    daher sinnvollerweise    gar   nicht  mehr    den Weg  ber
    'FilterADDHandle' gehen kann.

    Also legt VRAID.FLT intern  ein weiteres Device (BaseUnit) an, daá
    nur als Router zum ADD fungiert.  Allocate/Deallocate werden lokal
    behandelt     (das  echte Device  ist    ja   schon allokiert) und
    IOCC_CONFIGURATION ist ein   Spezialfall: es muá die  Arrays *und*
    die   Router melden  und  fr letzteres  sollte   man  sich in den
    BaseUnits' die Original-Unitinfo merken.

    Alle andere Requests gehen an den ADD,  VRAID.FLT muá *nicht* nach
    dem ADD wieder drankommen.

    Um  echte Host Drives    und BaseUnit's unterscheiden  zu  k”nnen,
    definieren wir 'UnitHandle' als Adresse  des Eintrags in 'apBDisk'
    oder 'apHDrv'.  Durch Vergleich mit den Adressen/L„ngen der beiden
    Tabellen   kommen wir dann  sehr  schnell  auf den  entsprechenden
    Eintrag.




Zu Testen vor jedem Release

    RAID1, Ausfall w„hrend Betrieb

	-> Array geht in Fail State.

    RAID1, Ausfall w„hrend Rebuild

	-> Array geht in Fail State

    RAID1, Ausfall w„hrend Build

	-> Array geht entweder in Error State oder -- bei zwei
	   Children -- Array stoppt den Build und definiert das andere
	   Child als Valid.





PHYSDEV zerst”ren

    Ein  biáchen kompliziert.   Entfernen wir nun  die VRAID Partition
    oder nicht?

    Wenn  wir sie  zerst”ren  und in VSetup  eine  seperate Klasse fr
    Dinger  erzeugen,  die im Treiber noch   PYHSDEVICE sind, aber die
    Partition schon verloren haben, dann  w„ren wir auf einer Linie it
    VORDev und  VOS2Disk.  Allerdings k”nnte  man  das auch  durch das
    Vorhandensein eines SEC_PHYSDEV triggern...

    -> Zur  Zeit ist  es  keine spezielle Klasse,  sondern ein VFRDev.
    Vom Icon evtl unsch”n, aber es funktioniert.



PHYSDEV Partition zerst”ren

    VSetup k”nnte jetzt auch die Partition eines PDEV entfernen, da
    alle PDEVs ja gleichzeitig auch OS/2 Disks sind!

    Mhmm, nicht wenn /!SHARE gesetzt ist

    Auáerdem ist das eine gr”áere nderung im VSetup:

    -> das VORDev muá sich bei der Erzeugung merken, wo die
       Partitionstabelle liegt, die zust„ndig ist.  Bei der Zerst”rung
       wird das abgefragt, ein VOS2Disk wird erzeugt (muá sowieso),
       dort ein PartTable Objekt fr die Tabelle erzeugt, eine
       passende Partition gefunden und zerst”rt.

    -> VFRDev muá sich sogar noch merken, auf welcher OS/2 Disk es
       liegt!  Dann darf fr die Disk extra ein VOS2Disk Objekt erzeugt
       werden und mit den weiteren Schritten wie oben fortgesetzt
       werden.

    Fazit: ist zu machen.   Aber lohnt der  Aufwand?  Wenn  mal jemand
    anders mitprogrammiert, dann ist  das evtl eine reizvolle  Aufgabe
    (ist  v”llig   seperat  vom   brigen   Leben   der   Programme zu
    implementieren und zu testen).



Ideensammlung, Technik

    Erste Implementation nur mit RAID 0 und „hnlich.  N„chste Stufe
    ist RAID 1 (kann man Build und Ausfall mit testen).  Danach RAID
    5.

    Kein gleichzeitiger Zugriff von OS/2 auf die beteiligten Platten
    (macht nur Probleme).

    -> dieser gleichzeitige Zugriff   ist   in den  heutigen    Zeiten
    zwingend n”tig.  Heutzutage  haben   die  meisten IDE  und   damit
    h”chstens 2 Platten frei (plus Boot und CD-ROM).


    Der Filter enth„lt keinerlei Cache, bzw nur die unbedingt zum
    Betrieb n”tigen Puffer (einstellbar, kann man ja an OS/2
    weiterliefern als maximale Gr”áe eines I/Os).

    Der Filter versteckt mehrere Platten und spiegelt OS/2 nur eine
    einzige vor.  Erkennung, welche gespiegelt werden sollen ber
    Marken auf den Platten.

    Diese Marken befinden sich auf den ersten Sektoren der Platten.
    Diese Sektoren werden vor OS/2 versteckt (Offset).

    Damit auf die Platten unter DOS o„ nicht einfach geschrieben wird
    oder ein Laufwerk gefunden wird (Buchstabensalat), enth„lt der
    echte Sektor 0 eine Partitionstabelle, die eine Partition
    beschreibt, die die komplette echte Platte beinhaltet.  Typ
    m”glichst abwegig.

    Es ist daher nicht m”glich, die  Bootplatte in's RAID einzubinden.
    RAID auf Partitionsebene ist nicht angestrebt.

    Ebenso ist Hot-/Warm-Plug kein erstes Ziel.  Die externen Probleme
    (Abschluá, St”rungen) sind ohne externe Maánahmen zu groá.

    RAID 1

	Jedes RAID-Laufwerk, das OS/2 kennt, bekommt eine Struktur.
	Aktive (und inaktive) I/Os fr dieses Laufwerk werden hier
	eingetragen.  Aktive I/Os an die Subplatten bekommen einen Link
	auf den Original I/O, der wiederum einen Counter enth„lt.  Ist
	dieser Counter auf 0, so wurden alle I/Os an Subplatten
	beendet und der OS/2 I/O ist fertig.

	Gelesen wird bei RAID 1 immer abwechselnd von einer der
	beteiligten Platten.  Es gibt sicherlich intelligentere
	Methoden, allerdings drfte das im Mittel keine groáen
	Vorteile bringen.

	Insbesondere bringt das gleichzeitige Lesen von beiden Platten
	vmtl mehr Probleme als Vorteile.


    RAID 0

	Evtl die zweite Implementation, Chaining finde ich pers”nlich
	v”llig sinnlos.  TW legt gerade Wert auf diesen Level.  Also 0
	und 1 implementieren, damit hat man auch ein gutes Design.


    RAID 4 oder 5

	Das ist natrlich der Verkaufsschlager.  RAID 0 ist nicht der
	Hit, RAID 1 kann der LAN Server schon so.

	Eine Besonderheit ist, daá der Filter hier deutlich mehr
	Speicher ben”tigt, da er alle Platten lesen muá.  Auch sind
	hier solche L”sungen wie von vortex im Vorteil, da sie einen
	groáen Cache haben, der fr die Berechnung einer neuen
	XOR-Summe gebraucht werden kann.

	Sp„testens hier w„re ein Wechsel auf 32bit angebracht.



Ideensammlung, Anderes

    Der Filter sollte entweder teure Shareware oder kommerziell sein.
    Leider erh”ht das den Aufwand, denn das Ding muá dann auch
    professionell aussehen.  Dafr kann sich die Arbeitszeit auch
    armortisieren.

    Wie stellt man aber bei Shareware die maximale L„nge der Testphase
    sicher?  Vmtl ber die eigenen Marken auf den Platten und die
    Echtzeituhr.  Vergl Gedanken zur Demoversion bei vortex.



Lieferumfang der Software

    Filter Treiber		<100KB

    Einrichtprogramm		open end

	IMHO ist es gnstiger, dieses Programm als VIO zu
	programmieren.  Damit hat man zumindest eine Chance, von
	Bootdisketten heraus das System aufzubauen.

	Einsatz von DOS-Programmen ist zu vermeiden, es gibt auch
	keinerlei Grund mehr fr ihre Existenz.

	-> Das Einrichtprogramm (VSetup) ist Presentation Manager.
	Die Arbeit der Kunden ist damit einfacher, als sich das Ganze
	durch Textbeschreibung vorzustellen.


    Dokumentation

	ASCII, Postscript

	-> INF ist kaum Aufwand, wenn man nicht zu fancy wird.
	Also streiche ASCII.


    Verkaufspreis der fertigen Software (RAID 0 1 4 5): 1000,- bis
    2000,-.  Der Preis ist zu hoch, H„lfte ist m”glich.

    Vergleiche c't (Leitz Ordner) Bericht ber neues Software RAID
    Produkt fr Windows.

    -> Preise sind zur Zeit 40,-, 70,- und 100,- EUR.



Design

    Fr jeden RAID-Level eine Sourcedatei.  Alle Zwischenstufen ber
    VRDEVICE Structuren verketten.  Siblings bilden ein zusammen
    Laufwerk, Child implementieren ihren Parent.  Nein, Tabelle pro
    Source aufbauen und verwalten.  Lebt doch nur bis zum Reboot.


    I/O werden als IORBs entgegengenommen und ber VRIOs
    weitergeleitet.  Im Prinzip k”nnte man auch die n”tigen Daten
    (Command, pSGList, cSGList, blk, bcnt) in den VRIO mit aufnehmen,
    aber durch die Referenz zum IORB haben wir ja etwas Speicher
    umsonst mitbekommen (ADD_WORKSPACE).  Muá allerdings zwischen den
    Childs geteilt werden. Mhmm...

    -> Kopiere die 4 Elemente des Requests zus„tzlich und machen den
    IORB unbrauchbar (indem PVOID verwendet wird).


    Es gibt pro Sourcedatei 1 ”ffentliche und 3 ber Zeiger im
    VRDHEADER exportierte Funktionen.  Die ”ffnentliche ist XxxxCreate
    und erzeigt eine Struktur, dessen Aufbau nur dieser Source kennt
    (bis auf Header).  Die ber Zeiger exportierten Funktionen sind:

    worker	nimmt I/Os entgegen und verteilt sie an Childs.  I/Os
		k”nnen den Datenbereich oder den Admin-Bereich
		betreffen (gibt Flags).

    notify	wird von Childs aufgerufen, wenn sie einen I/O
		fertiggestellt haben

    update	wird mit dem aktuellen Inhalt des Admin-Sektors mit
		der eigenen Konfigugaration aufgerufen.  Diese
		Funktion kann ihn dann modifizieren und zurckgeben,
		ob er geschrieben werden soll.

		Diese Funktionalit„t sollte regelm„áig genutzt werden.


    Die letzte Notify Routinen muá dann wissen, wie der I/O zu beenden
    ist.  Z.Z. wenn *notify == NULL, dann HostdriveNotify() aufrufen.
    Vielleicht kann der den IORB analysieren und/oder ein Proc/Block
    Schema beenden?


    TYPE_SINGLE tun den tats„chlichen I/O.  Die dazu n”tigen Werte
    befinden sich in einer zugekettenen PHYSDEVICE struktur.  Alle
    Child benachrichten ihre Parent vom den Ergebnis der Aktion.  Alle
    Childs bekommen als Parameter 'self', den IORB und einen Offset in
    Sektoren.

    Anmelden an OS/2 muá dafr sorgen, daá ein IORB passend teilbar
    ist (S/G List splitbar).

    Multi-I/O bis in unterste Schichten ist zwingend notwendig, sonst
    bekommen wir das Mirror-Update nie auf die Reihe!

    Fr jedes PHYSDEVICE wird regelm„áig (bei Nichtbenutzung) ein I/O
    kreiert, der die Verfgbarkeit testet.  Kollidiert allerdings mit
    DSKSleep. :-(

    Was passiert, wenn der Speicher fr weitere Operationen nicht
    ausreicht (kein VRIO oder IORB)?  Vielleicht in eine Queue, die
    via Timer weiterbearbeitet wird?  Dann aber doch alle VRIO einer
    Routine allokieren und hinterher an die Childs schicken.  Diese
    versuchen das gleiche, aber speichern den VRIO wenn es nicht klappt.

 *	Alle neuen VRIO allokieren und dann erst zu den Child schicken?
 *	Wenn kein Speicher mehr, dann Kommando zurck und letzendlich
 *	kommt der original IORB wieder in einen Queue?
 *	Geht nicht, tieferliegende Routinen brauchen ja ebenfalls noch
 *	Speicher!
 *
 *	L”sung: alle VRIO allokieren.  Wenn das ging, dann alle an die
 *	tieferen Routinen absetzen.  Wenn nicht, diesen VRIO in eine Liste
 *	fr sp„tere Abarbeitung einfgen.
 *	Tiefere Routinen machen es ebenso, damit gibt es nur eine Liste:
 *	die mit VRIOs, die noch gestartet werden mssen. Das zust„ndige
 *	VRDEVICE ist dort schon eingetraten und der worker auch!
